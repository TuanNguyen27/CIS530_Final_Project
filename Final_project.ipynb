{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 6.29156152  6.76178628  6.14677343  7.22381497  6.64853368  6.24947411\n",
      "  5.89423931  6.92196827  5.72816342  5.2933439   4.90443334  6.42288704\n",
      "  6.65299279  5.63073879  5.76484799  5.58763     4.94572154  6.21809424\n",
      "  6.40143522  5.40406123  6.15648351  6.06886051  6.01806399  6.49697383\n",
      "  5.62167063  5.37455909  5.68678749  7.22077973  5.05688485  5.7236115\n",
      "  6.8131959   4.71698617  6.62397748  7.26827795  6.64853368  5.77256109\n",
      "  5.21232941  6.63520594  7.03863292  5.68474825  6.61285218  6.86125146\n",
      "  6.54764056  4.3435283   6.86569413  6.51390158  5.88195813  6.62545321\n",
      "  5.49530972  5.7785789 ]\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python3\n",
    "import string\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import wordnet as wn\n",
    "from os import listdir, system, popen\n",
    "from os.path import isfile, join, basename\n",
    "from itertools import chain\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.svm import SVR\n",
    "from nltk.util import ngrams\n",
    "from math import log, log2\n",
    "import itertools\n",
    "from collections import Counter, defaultdict\n",
    "from nltk.tree import Tree\n",
    "# import pdb\n",
    "import re\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from scipy.spatial.distance import cosine\n",
    "import os\n",
    "\n",
    "# ntuan_home = \"/Users/admin/Documents/cis530/final/\"\n",
    "ntuan_home = \"\"\n",
    "# data_path = \"/Users/admin/Documents/cis530/final/\"\n",
    "data_path = \"\"\n",
    "train_path = data_path + \"data/project_train.txt\"\n",
    "test_path = data_path + \"data/project_test.txt\"\n",
    "label_path = data_path + \"data/project_train_scores.txt\"\n",
    "optional_train_path = data_path + \"data/optional_training\"\n",
    "optional_labels = data_path + \"data/optional_training/optional_project_train_scores.txt\"\n",
    "\n",
    "#Credit: http://stackoverflow.com/questions/12093940/reading-files-in-a-particular-order-in-python\n",
    "ntuan_numbers = re.compile(r'(\\d+)')\n",
    "\n",
    "def numericalSort(value):\n",
    "    parts = ntuan_numbers.split(value)\n",
    "    parts[1::2] = map(int, parts[1::2])\n",
    "    return parts\n",
    "\n",
    "def get_all_files(directory):\n",
    "    return sorted([join(directory, f) for f in listdir(directory)], key = numericalSort)\n",
    "\n",
    "def standardize(rawexcerpt):\n",
    "    return [w for w in word_tokenize(rawexcerpt.lower())]\n",
    "\n",
    "def load_file_excerpts(filepath):\n",
    "    excerpts = []\n",
    "    file_i = open(filepath)\n",
    "    for line in file_i.read().strip().split('\\n'):\n",
    "        excerpts.append(standardize(line.strip()))\n",
    "    file_i.close()\n",
    "    return excerpts\n",
    "\n",
    "def flatten(listoflists):\n",
    "    return list(chain.from_iterable(listoflists))\n",
    "\n",
    "def load_optional_train(optional_train_files):\n",
    "    excerpts = []\n",
    "    for path_i in optional_train_files:\n",
    "        file_i = open(path_i)\n",
    "\n",
    "def nsyl(word):\n",
    "    vowels = \"aeiouy\"\n",
    "    numVowels = 0\n",
    "    lastWasVowel = False\n",
    "    for wc in word:\n",
    "        foundVowel = False\n",
    "        for v in vowels:\n",
    "            if v == wc:\n",
    "                if not lastWasVowel: numVowels+=1   #don't count diphthongs\n",
    "                foundVowel = lastWasVowel = True\n",
    "                break\n",
    "        if not foundVowel:  #If full cycle and no vowel found, set lastWasVowel to false\n",
    "            lastWasVowel = False\n",
    "    if len(word) > 2 and word[-2:] == \"es\": #Remove es - it's \"usually\" silent (?)\n",
    "        numVowels-=1\n",
    "    elif len(word) > 1 and word[-1:] == \"e\":    #remove silent e\n",
    "        numVowels-=1\n",
    "    return numVowels\n",
    "\n",
    "def build_feature(train_data_path, stopwords, nytimes):\n",
    "    curr_file = open(train_data_path)\n",
    "    train_data = curr_file.read().strip().split('\\n')\n",
    "    training_num = len(train_data)\n",
    "    feature_vec = []\n",
    "    for i in range(training_num):\n",
    "        feature_vec_i = []\n",
    "        excerpt = train_data[i]\n",
    "        sents = sent_tokenize(excerpt)\n",
    "        total_sent_length = sum([len(sent) for sent in sents])\n",
    "        num_sents = len(sents)\n",
    "        \n",
    "        #count # words in each excerpt\n",
    "        words = flatten([word_tokenize(sent) for sent in sents])\n",
    "        words = [word for word in words if word not in stopwords]\n",
    "        #words = excerpt\n",
    "\n",
    "        #word count frequency\n",
    "        counts = Counter(words)\n",
    "        num_words = len(words)\n",
    "        \n",
    "        #unigram distrbution\n",
    "        unigram = {word:float(counts[word])/num_words for word in counts}\n",
    "        entropy = -sum([unigram[word_i]*log2(unigram[word_i]) for word_i in unigram])\n",
    "\n",
    "        #words with length > 13 chars\n",
    "        extra_long = len([word for word in words if len(word) > 13])\n",
    "        \n",
    "        #type-token ratio\n",
    "        type_tok = len(set(words))/len(words)\n",
    "        \n",
    "        #percent of words not in nyt vocab\n",
    "        nyt_frac = len([word for word in words if word not in nytimes])/len(words)\n",
    "        \n",
    "        #word length distribution\n",
    "        word_len = np.array([len(word) for word in words])\n",
    "        \n",
    "        #avg length of a word in excerpt\n",
    "        avg_word_len = np.mean(word_len)\n",
    "        \n",
    "        #median length of a word in excerpt\n",
    "        median_word_len = np.median(word_len)\n",
    "        \n",
    "        #following steps do not work because excerpt is alr tokenized before being sent_tokenize\n",
    "        num_syl = sum([nsyl(word.lower()) for word in words])\n",
    "        avg_syl_per_word = float(num_syl)/num_words\n",
    "        avg_word_per_sent = float(num_words)/num_sents\n",
    "        FL_score = 206.835 - 1.015*avg_word_per_sent - 84.6*avg_syl_per_word\n",
    "\n",
    "        feature_vec.append([entropy, extra_long, type_tok, nyt_frac, avg_word_len, median_word_len, FL_score])\n",
    "\n",
    "    return np.asarray(feature_vec)\n",
    "\n",
    "\n",
    "train_data = load_file_excerpts(train_path)\n",
    "test_data = load_file_excerpts(test_path)\n",
    "# optional_train_files = get_all_files(optional_train_path)[:-1]\n",
    "\n",
    "curr_file = open(ntuan_home + \"stopwords.txt\")\n",
    "stopwords = set(curr_file.read().strip().split('\\n'))\n",
    "curr_file.close()\n",
    "\n",
    "#build a vocab of all words in nytimes.txt, excluding stopwords\n",
    "nytimes = flatten(load_file_excerpts(ntuan_home + \"nytimes.txt\"))\n",
    "nytimes = set([word_i for word_i in nytimes if word_i not in stopwords])\n",
    "nytimes = set(nytimes)\n",
    "curr_file.close()\n",
    "\n",
    "X_train = build_feature(train_data, stopwords, nytimes)\n",
    "X_test  = build_feature(test_data, stopwords, nytimes)\n",
    "labels_file = open(label_path)\n",
    "Y_train = np.asarray([int(line_i) for line_i in labels_file.read().strip().split('\\n')])\n",
    "labels_file.close()\n",
    "\n",
    "clf = SVR(C=1.0, epsilon=0.2)\n",
    "clf.fit(X_train, Y_train)\n",
    "pred = clf.predict(X_test)\n",
    "print(pred)\n",
    "\n",
    "# labels_file = open(optional_labels)\n",
    "# optional_labels = {line_i.split()[0]: (1-float(line_i.split()[1])) for line_i in labels_file.read().strip().split('\\n')}\n",
    "# labels_file.close()\n",
    "# print(optional_labels)\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.],\n",
       "       [ 11.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  0.]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "x = np.zeros([5, 1])\n",
    "x[1] = 11\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
