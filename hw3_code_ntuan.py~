#!/usr/bin/python3
from nltk import word_tokenize, sent_tokenize
from os import listdir, system, popen
from os.path import isfile, join, basename
from itertools import chain
import numpy, itertools
from math import log, log2, sqrt
from collections import Counter, defaultdict

# ntuan_home = "/Users/admin/Documents/cis530/hw3/data/train/"
ntuan_home = "/home1/n/ntuan/cis530/hw3/data/train/"
ntuan_home_2 = "/home1/n/ntuan/cis530/hw3/"

def get_all_files(directory):
    return [join(directory, f) for f in listdir(directory) if isfile(join(directory, f)) and f.lower().endswith('.txt')]

def write_to_file(list_of_stuff, file_name):
    #write each thing in list_of_stuff to one line in <file_name>.txt
    if not isfile(join(ntuan_home_2, file_name)):
        with open(ntuan_home_2 + file_name, 'w') as csvfile:
            csvfile.writelines(str(item_i) + '\n' for item_i in list_of_stuff)

def run_Stanford_coreNLP():
    train_dir = ntuan_home_2 + "data/train/"
    test_dir = ntuan_home_2 + "data/test/"
    train_files = get_all_files(train_dir)
    test_files = get_all_files(test_dir)
    write_to_file(train_files, "train_file_list.txt")
    write_to_file(test_files, "test_file_list.txt")
    file_list = [ntuan_home_2 + "train_file_list.txt", ntuan_home_2 + "test_file_list.txt"]
    for file_i in file_list:
        system("cd /home1/n/ntuan/cis530/hw3/stanford-corenlp-2012-07-09/;\
        java -cp stanford-corenlp-2012-07-09.jar:stanford-corenlp-2012-07-06-models.jar:\
        xom.jar:joda-time.jar -Xmx3g edu.stanford.nlp.pipeline.StanfordCoreNLP \
        -annotators tokenize,ssplit,pos,lemma,ner,parse -filelist "
          + file_i + " -outputDirectory " + ntuan_home_2)

def extract_pos_tags(xml_directory):
#returns all the unique pos tags from all documents in xml_directory.
    xml = get_all_files(xml_directory)
    tags = []
    for file_i in xml:
        curr_file = open(file_i)
        tags += [line.strip().split("POS")[1][1:-2] for line in curr_file.read().split('\n') if "<POS>" in line]
        curr_file.close()
    tags = sorted(list(set(tags)))
    write_to_file(tags, "hw3_4-1.txt")
    return tags

def map_pos_tags(xml_filename, pos_tag_list):
#takes an xml file path and the list of known POS tags (output from 4.1) as input and returns a vector in the feature space of the known POS tag list.
    curr_file = open(xml_filename)
    tags = [line.strip().split("POS")[1][1:-2] for line in curr_file.read().split('\n') if "<POS>" in line]
    curr_file.close()
    count = Counter(tags)
    num_tok = len(tags)
    num_tag = len(pos_tag_list)
    pos_tags_vec = [0]*num_tag
    for i in range(num_tag):
        pos_tags_vec = float(count[pos_tag_list[i]])/num_tok
    return pos_tags_vec

def map_universal_tags(ptb_pos_feat_vector, pos_tag_list, ptb_google_mapping, universal_tag_list):
#The function returns a vector in the feature space of the universal tag list.
#It returns a list of integers with the same size as universal tag list.
#Each element in the returned list is equal to the fraction of tokens in the text (represented by the input vector)
#with the corresponding universal POS tag in universal tag list.
    ggl_vec = [0]*len(universal_tag_list)
    N = len(ptb_pos_feat_vector)
    for i in range(N):
        pos_tag = pos_tag_list[i]
        ggl_tag = ptb_google_mapping[pos_tag]
        index = universal_tag_list.index(ggl_tag)
        ggl_vec[index] = ptb_pos_feat_vector[i]
    return ggl_vec

def extract_ner_tags(xml_directory):
    xml = get_all_files(xml_directory)
    tags = []
    for file_i in xml:
        curr_file = open(file_i)
        tags += [line.strip().split("NER")[1][1:-2] for line in curr_file.read().split('\n') if "<NER>" in line]
        curr_file.close()
    tags = sorted(list(set(tags)))
    write_to_file(tags, "hw3_5-1.txt")
    return tags

def map_named_entity_tags(xml_filename, entity_list):
    curr_file = open(xml_filename)
    tags = [line.strip().split("NER")[1][1:-2] for line in curr_file.read().split('\n') if "<NER>" in line]
    curr_file.close()
    count = Counter(tags)
    num_tok = len(tags)
    num_tag = len(entity_list)
    pos_tags_vec = [0]*num_tag
    for i in range(num_tag):
        pos_tags_vec = float(count[entity_list[i]])/num_tok
    return pos_tags_vec

def extract_dependencies(xml_directory):
    xml = get_all_files(xml_directory)
    read_flag = False
    dep_tag = []
    for file_i in xml:
        curr_file = open(file_i)
        for line in curr_file.read().split("\n"):
            line = line.strip()
            if read_flag and "dep type" in line:
                dep_tag.append(line.split('"')[-2])
            if line == "</basic-dependencies>":
                read_flag = False
            if line == "<basic-dependencies>":
                read_flag = True
        curr_file.close()
        read_flag = False
    dep_tag = sorted(list(set(dep_tag)))
    write_to_file(dep_tag, "hw3_6-1.txt")
    return tags

# def map_dependencies(xml_filename, dependency_list)

# def extract_prod_rules(xml_directory)

# def map_prod_rules(xml_filename, rules_list)

# def map_brown_clusters(xml_file_path, cluster_code_list, word_cluster_mapping)

# def generate_cluster_codes(brown_file_path)

# def generate_word_cluster_mapping(brown_file_path)

# def createPOSFeat(xml_dir, pos_tag_list)

# def createUniversalPOSFeat(pos_feat_2D_array, pos_tag_list, ptb_google_mapping, universal_tag_list)

# def createNERFeat(xml_dir, entity_list)

# def createDependencyFeat(xml_dir, dependency_list)

# def createSyntaticProductionFeat(xml_dir, rules_list)

# def createBrownClusterFeat(xml_dir, cluster_code_list, word_cluster_mapping)

if __name__ == "__main__":
#     curr_file = open("/home1/n/ntuan/cis530/hw3/data/en-ptb.map")
#     ptb_google_mapping = defaultdict(" ")
#     for line in curr_file.read().split("\n"):
#         if line.split():
#             ptb_google_mapping[line.split()[0]] = line.split()[1]
#     curr_file.close()
    # run_Stanford_coreNLP()

    extract_pos_tags("/home1/n/ntuan/cis530/hw3/train_parse")
    extract_dependencies("/home1/n/ntuan/cis530/hw3/train_parse")
    extract_ner_tags("/home1/n/ntuan/cis530/hw3/train_parse")
